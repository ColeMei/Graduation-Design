{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61b67bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574b7fe6",
   "metadata": {},
   "source": [
    "## 1. Extract the review data from csv and save it as txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29a4e4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file paths to read from\n",
    "file_paths = ['../data/北京apm.csv', '../data/王府中環.csv', '../data/王府井百货.csv', '../data/东方新天地.csv']\n",
    "\n",
    "# Loop over each file, extract the data from the 'review' column and save it to a text file\n",
    "for i, path in enumerate(file_paths):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(path, index_col=0)  # Set the index_col to 0 to use the first column as the index\n",
    "    # Define the path for the output text file\n",
    "    txt_path = f'../data/review/reviews_{i+1}.txt'\n",
    "    # Write the review data to the output text file\n",
    "    with open(txt_path, 'w') as f:\n",
    "        for j, review in enumerate(df['review']):\n",
    "            # Clean up the text data by removing special characters\n",
    "            review = re.sub('[\\n\\r\\t ]+', '', review)\n",
    "            # Write the review data to the output text file\n",
    "            f.write(str(review) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bda8013",
   "metadata": {},
   "source": [
    "### Merge into a new txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ee4815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file paths to read from\n",
    "txt_paths = ['../data/review/reviews_1.txt', '../data/review/reviews_2.txt', '../data/review/reviews_3.txt', '../data/review/reviews_4.txt']\n",
    "\n",
    "# Define the path for the output merged text file\n",
    "merged_txt_path = '../data/review/review_merged.txt'\n",
    "\n",
    "# Open the output file for writing\n",
    "with open(merged_txt_path, 'w') as merged_file:\n",
    "    # Loop over each input file\n",
    "    for txt_path in txt_paths:\n",
    "        # Open the input file for reading\n",
    "        with open(txt_path, 'r') as input_file:\n",
    "            # Loop over each line in the input file\n",
    "            for line in input_file:\n",
    "                # Write the line to the output file\n",
    "                merged_file.write(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bb27d7",
   "metadata": {},
   "source": [
    "## 2. Remove stop words, punctuation, and emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89eca84",
   "metadata": {},
   "source": [
    "### Method without splitting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e71b74c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "# Define the file paths to read from\n",
    "file_paths = ['../data/review/reviews_1.txt', '../data/review/reviews_2.txt', '../data/review/reviews_3.txt', '../data/review/reviews_4.txt']\n",
    "\n",
    "# Define the file paths to save to\n",
    "save_paths = ['../data/processed/processed_1.txt', '../data/processed/processed_2.txt', '../data/processed/processed_3.txt', '../data/processed/processed_4.txt']\n",
    "\n",
    "# Define the stopwords dictionaries to use\n",
    "stopwords_paths = ['stopwords/baidu_stopwords.txt', 'stopwords/cn_stopwords.txt', 'stopwords/hit_stopwords.txt', 'stopwords/scu_stopwords.txt']\n",
    "\n",
    "# Define a function to remove punctuation marks from a string\n",
    "def remove_punctuation(text):\n",
    "    # Remove all punctuation marks\n",
    "    return re.sub('[^\\w\\s,，。！？\\uff00-\\uffef]', '', text)\n",
    "\n",
    "# Loop over each file, read the text, remove stopwords and punctuation marks/emojis, and save to a new file\n",
    "for i, path in enumerate(file_paths):\n",
    "    # Read the text from the input file\n",
    "    with open(path, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Load the custom stopwords dictionary\n",
    "    stopwords_path = stopwords_paths[i]\n",
    "    stopwords = set()\n",
    "    if os.path.exists(stopwords_path):\n",
    "        with open(stopwords_path, 'r') as f:\n",
    "            for line in f:\n",
    "                stopwords.add(line.strip())\n",
    "\n",
    "    # Replace emojis with their textual representation and remove remaining punctuation marks\n",
    "    processed_text = remove_punctuation(emoji.emojize(text))\n",
    "\n",
    "    # Remove stopwords from the text\n",
    "    processed_text = ' '.join([word for word in processed_text.split() if word not in stopwords])\n",
    "\n",
    "    # Write the processed text to the output file\n",
    "    save_path = save_paths[i]\n",
    "    with open(save_path, 'w') as f:\n",
    "        f.write(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3fd2d9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "# Define the file paths to read from and write to\n",
    "file_paths = ['../data/review/reviews_1.txt', '../data/review/reviews_2.txt', '../data/review/reviews_3.txt', '../data/review/reviews_4.txt']\n",
    "output_paths = ['../data/processed/processed_1.txt', '../data/processed/processed_2.txt', '../data/processed/processed_3.txt', '../data/processed/processed_4.txt']\n",
    "\n",
    "\n",
    "# Define the paths to your custom stop word dictionaries\n",
    "stopwords_paths = ['stopwords/baidu_stopwords.txt', 'stopwords/cn_stopwords.txt', 'stopwords/hit_stopwords.txt', 'stopwords/scu_stopwords.txt']\n",
    "\n",
    "# Combine all stop word dictionaries into one set\n",
    "stopwords = set()\n",
    "for path in stopwords_paths:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            stopwords.add(line.strip())\n",
    "\n",
    "# Define a function to process each review text\n",
    "def process_text(text):\n",
    "    # Remove all punctuation except for emoticons\n",
    "    text = re.sub(r'[^\\w\\s\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF]+', '', text)\n",
    "    # Remove stop words\n",
    "    for word in stopwords:\n",
    "        text = text.replace(word, '')\n",
    "    # Return the processed text as a string\n",
    "    return text.strip()\n",
    "\n",
    "# Loop over each file, process the review text and save it to a new text file\n",
    "for i, input_path in enumerate(file_paths):\n",
    "    with open(input_path, 'r', encoding='utf-8') as f_input, open(output_paths[i], 'w', encoding='utf-8') as f_output:\n",
    "        for line in f_input:\n",
    "            # Process the review text\n",
    "            processed_text = process_text(line.strip())\n",
    "            # Write the processed text to the output file\n",
    "            f_output.write(processed_text + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe21adf7",
   "metadata": {},
   "source": [
    "### Method of splitting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "851dc3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import jieba\n",
    "\n",
    "# Define the file paths to read from\n",
    "file_paths = ['../data/review/reviews_1.txt', '../data/review/reviews_2.txt', '../data/review/reviews_3.txt', '../data/review/reviews_4.txt']\n",
    "\n",
    "# Define the file paths to save to\n",
    "save_paths = ['../data/processed/split_1.txt', '../data/processed/split_2.txt', '../data/processed/split_3.txt', '../data/processed/split_4.txt']\n",
    "\n",
    "# Define the stopwords dictionaries to use\n",
    "stopwords_paths = ['stopwords/baidu_stopwords.txt', 'stopwords/cn_stopwords.txt', 'stopwords/hit_stopwords.txt', 'stopwords/scu_stopwords.txt']\n",
    "\n",
    "# Define a function to remove punctuation marks and emojis from a string\n",
    "def remove_punctuation(text):\n",
    "    # Remove all punctuation marks and emojis\n",
    "    return re.sub('[^\\w\\s]+', '', text)\n",
    "\n",
    "# Loop over each file, read the text, remove stopwords and punctuation marks/emojis, and save to a new file\n",
    "for i, path in enumerate(file_paths):\n",
    "    # Read the text from the input file\n",
    "    with open(path, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Load the custom stopwords dictionary\n",
    "    stopwords_path = stopwords_paths[i]\n",
    "    stopwords = set()\n",
    "    if os.path.exists(stopwords_path):\n",
    "        with open(stopwords_path, 'r') as f:\n",
    "            for line in f:\n",
    "                stopwords.add(line.strip())\n",
    "\n",
    "    # Tokenize the text using jieba\n",
    "    words = jieba.cut(text)\n",
    "\n",
    "    # Remove stopwords and punctuation marks/emojis from the text\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords and word.strip():\n",
    "            processed_words.append(word)\n",
    "\n",
    "    processed_text = ' '.join(processed_words)\n",
    "    processed_text = remove_punctuation(processed_text.strip())\n",
    "\n",
    "    # Write the processed text to the output file\n",
    "    save_path = save_paths[i]\n",
    "    with open(save_path, 'w') as f:\n",
    "        f.write(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3483b9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
