{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cbe1bba",
   "metadata": {},
   "source": [
    "# Part 1. Calculation and Analysis of Sentiment Value of Review Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06ec2db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from snownlp import SnowNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed265f6",
   "metadata": {},
   "source": [
    "## 1. Building an sentiment lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f94efa",
   "metadata": {},
   "source": [
    "### Dictionary for degree adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc2e6d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define weights for different degree adverbs\n",
    "category1 = {\n",
    "    '绝对': 2,\n",
    "    '绝对化': 2,\n",
    "    '绝对性': 2,\n",
    "    '绝顶': 2,\n",
    "    '万分': 2,\n",
    "    '不得了': 2,\n",
    "    '不可开交': 2,\n",
    "    '不亦乐乎': 2,\n",
    "    '不折不扣': 2,\n",
    "    '彻头彻尾': 2,\n",
    "    '充分': 2,\n",
    "    '到头': 2,\n",
    "    '地地道道': 2,\n",
    "    '非常': 2,\n",
    "    '极': 2,\n",
    "    '极度': 2,\n",
    "    '极端': 2,\n",
    "    '极其': 2,\n",
    "    '极为': 2,\n",
    "    '截然': 2,\n",
    "    '尽': 2,\n",
    "    '惊人地': 2,\n",
    "    '绝': 2,\n",
    "    '刻骨': 2,\n",
    "    '酷': 2,\n",
    "    '满': 2,\n",
    "    '满贯': 2,\n",
    "    '满心': 2,\n",
    "    '莫大': 2,\n",
    "    '奇': 2,\n",
    "    '入骨': 2,\n",
    "    '甚为': 2,\n",
    "    '十二分': 2,\n",
    "    '十分': 2,\n",
    "    '十足': 2,\n",
    "    '死': 2,\n",
    "    '滔天': 2,\n",
    "    '痛': 2,\n",
    "    '透': 2,\n",
    "    '完全': 2,\n",
    "    '完完全全': 2,\n",
    "    '万': 2,\n",
    "    '万般': 2,\n",
    "    '无比': 2,\n",
    "    '无度': 2,\n",
    "    '无可估量': 2,\n",
    "    '无以复加': 2,\n",
    "    '无以伦比': 2,\n",
    "    '要命': 2,\n",
    "    '要死': 2,\n",
    "    '已极': 2,\n",
    "    '已甚': 2,\n",
    "    '异常': 2,\n",
    "    '逾常': 2,\n",
    "    '贼': 2,\n",
    "    '之极': 2,\n",
    "    '之至': 2,\n",
    "    '至极': 2,\n",
    "    '卓绝': 2,\n",
    "    '最为': 2,\n",
    "    '佼佼': 2,\n",
    "    '郅': 2,\n",
    "    '綦': 2,\n",
    "    '齁': 2,\n",
    "    '最': 2,\n",
    "}\n",
    "\n",
    "category2 = {\n",
    "    '不为过': 1.8,\n",
    "    '超': 1.8,\n",
    "    '超额': 1.8,\n",
    "    '超外差': 1.8,\n",
    "    '超微结构': 1.8,\n",
    "    '超物质': 1.8,\n",
    "    '出头': 1.8,\n",
    "    '多': 1.8,\n",
    "    '浮': 1.8,\n",
    "    '过': 1.8,\n",
    "    '过度': 1.8,\n",
    "    '过分': 1.8,\n",
    "    '过火': 1.8,\n",
    "    '过劲': 1.8,\n",
    "    '过了头': 1.8,\n",
    "    '过猛': 1.8,\n",
    "    '过热': 1.8,\n",
    "    '过甚': 1.8,\n",
    "    '过头': 1.8,\n",
    "    '过于': 1.8,\n",
    "    '过逾': 1.8,\n",
    "    '何止': 1.8,\n",
    "    '何啻': 1.8,\n",
    "    '开外': 1.8,\n",
    "    '苦': 1.8,\n",
    "    '老': 1.8,\n",
    "    '偏': 1.8,\n",
    "    '强': 1.8,\n",
    "    '溢': 1.8,\n",
    "    '忒': 1.8\n",
    "}\n",
    "\n",
    "category3 = {\n",
    "    \"不过\": 1.5,\n",
    "    \"不少\": 1.5,\n",
    "    \"不胜\": 1.5,\n",
    "    \"惨\": 1.5,\n",
    "    \"沉\": 1.5,\n",
    "    \"沉沉\": 1.5,\n",
    "    \"出奇\": 1.5,\n",
    "    \"大为\": 1.5,\n",
    "    \"多\": 1.5,\n",
    "    \"多多\": 1.5,\n",
    "    \"多加\": 1.5,\n",
    "    \"多么\": 1.5,\n",
    "    \"分外\": 1.5,\n",
    "    \"格外\": 1.5,\n",
    "    \"够瞧的\": 1.5,\n",
    "    \"够戗\": 1.5,\n",
    "    \"好不\": 1.5,\n",
    "    \"何等\": 1.5,\n",
    "    \"很\": 1.5,\n",
    "    \"很是\": 1.5,\n",
    "    \"坏\": 1.5,\n",
    "    \"可\": 1.5,\n",
    "    \"老\": 1.5,\n",
    "    \"老大\": 1.5,\n",
    "    \"良\": 1.5,\n",
    "    \"颇\": 1.5,\n",
    "    \"颇为\": 1.5,\n",
    "    \"甚\": 1.5,\n",
    "    \"实在\": 1.5,\n",
    "    \"太\": 1.5,\n",
    "    \"太甚\": 1.5,\n",
    "    \"特\": 1.5,\n",
    "    \"特别\": 1.5,\n",
    "    \"尤\": 1.5,\n",
    "    \"尤其\": 1.5,\n",
    "    \"尤为\": 1.5,\n",
    "    \"尤以\": 1.5,\n",
    "    \"远\": 1.5,\n",
    "    \"着实\": 1.5,\n",
    "    \"曷\": 1.5,\n",
    "    \"碜\": 1.5\n",
    "}\n",
    "\n",
    "category4 = {\n",
    "    '大不了': 1.2,\n",
    "    '多': 1.2,\n",
    "    '更': 1.2,\n",
    "    '更加': 1.2,\n",
    "    '更进一步': 1.2,\n",
    "    '更为': 1.2,\n",
    "    '还': 1.2,\n",
    "    '还要': 1.2,\n",
    "    '较': 1.2,\n",
    "    '较比': 1.2,\n",
    "    '较为': 1.2,\n",
    "    '进一步': 1.2,\n",
    "    '那般': 1.2,\n",
    "    '那么': 1.2,\n",
    "    '那样': 1.2,\n",
    "    '强': 1.2,\n",
    "    '如斯': 1.2,\n",
    "    '益': 1.2,\n",
    "    '益发': 1.2,\n",
    "    '尤甚': 1.2,\n",
    "    '逾': 1.2,\n",
    "    '愈': 1.2,\n",
    "    '愈 ... 愈': 1.2,\n",
    "    '愈发': 1.2,\n",
    "    '愈加': 1.2,\n",
    "    '愈来愈': 1.2,\n",
    "    '愈益': 1.2,\n",
    "    '远远': 1.2,\n",
    "    '越 ... 越': 1.2,\n",
    "    '越发': 1.2,\n",
    "    '越加': 1.2,\n",
    "    '越来越': 1.2,\n",
    "    '越是': 1.2,\n",
    "    '这般': 1.2,\n",
    "    '这样': 1.2,\n",
    "    '足': 1.2,\n",
    "    '足足': 1.2,\n",
    "}\n",
    "\n",
    "category5 = {\n",
    "    '点点滴滴': 0.8,\n",
    "    '多多少少': 0.8,\n",
    "    '怪': 0.8,\n",
    "    '好生': 0.8,\n",
    "    '还': 0.8,\n",
    "    '或多或少': 0.8,\n",
    "    '略': 0.8,\n",
    "    '略加': 0.8,\n",
    "    '略略': 0.8,\n",
    "    '略微': 0.8,\n",
    "    '略为': 0.8,\n",
    "    '稍': 0.8,\n",
    "    '稍稍': 0.8,\n",
    "    '稍微': 0.8,\n",
    "    '稍为': 0.8,\n",
    "    '稍许': 0.8,\n",
    "    '挺': 0.8,\n",
    "    '未免': 0.8,\n",
    "    '相当': 0.8,\n",
    "    '些': 0.8,\n",
    "    '些微': 0.8,\n",
    "    '些小': 0.8,\n",
    "    '一点': 0.8,\n",
    "    '一点儿': 0.8,\n",
    "    '一些': 0.8,\n",
    "    '有点': 0.8,\n",
    "    '有点儿': 0.8,\n",
    "    '有些': 0.8\n",
    "}\n",
    "\n",
    "category6 = {\n",
    "    \"半点\": 0.5,\n",
    "    \"不大\": 0.5,\n",
    "    \"不丁点儿\": 0.5,\n",
    "    \"不甚\": 0.5,\n",
    "    \"不怎么\": 0.5,\n",
    "    \"聊\": 0.5,\n",
    "    \"没怎么\": 0.5,\n",
    "    \"轻度\": 0.5,\n",
    "    \"弱\": 0.5,\n",
    "    \"丝毫\": 0.5,\n",
    "    \"微\": 0.5,\n",
    "    \"相对\": 0.5\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7985e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of 6 dictionaries\n",
    "dict_list = [category1, category2, category3, category4, category5, category6]\n",
    "\n",
    "# Merge all dictionaries into a single dictionary\n",
    "merged_dict = {}\n",
    "for d in dict_list:\n",
    "    merged_dict.update(d)\n",
    "\n",
    "# Save the merged dictionary to a txt file\n",
    "# Open the file in write mode, encode with utf-8 and write as a json object\n",
    "with open('sentiment_dic/adv_dic.txt', 'w', encoding='utf-8') as f:\n",
    "    json.dump(merged_dict, f, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1539f270",
   "metadata": {},
   "source": [
    "### Dictionary for negative words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac12c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read a txt file and return its contents as a list of lines\n",
    "def read_txt_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read().splitlines()\n",
    "\n",
    "# Read words from negative_dic.txt file\n",
    "neg_words = read_txt_file('sentiment_dic/base/negative_dic.txt')\n",
    "\n",
    "# Convert the list of words into a dictionary and set the value of each word as 1\n",
    "neg_dict = {word: 1 for word in neg_words}\n",
    "\n",
    "# Save the negative dictionary to a txt file\n",
    "# Open the file in write mode, encode with utf-8 and write as a json object\n",
    "with open('sentiment_dic/neg_dic.txt', 'w', encoding='utf-8') as f:\n",
    "    json.dump(neg_dict, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b5bc1",
   "metadata": {},
   "source": [
    "### Dictionary for emotional words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9226945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt_file(file_path, encoding):\n",
    "    with open(file_path, 'r', encoding=encoding) as f:\n",
    "        return f.read().splitlines()\n",
    "\n",
    "# Define the file paths and encodings for each sentiment word file\n",
    "SENTIMENT_FILES = [\n",
    "    ('sentiment_dic/base/ntusd_dic/NTUSD_negative.txt', 'utf-16le'),\n",
    "    ('sentiment_dic/base/ntusd_dic/NTUSD_positive.txt', 'utf-16le'),\n",
    "    ('sentiment_dic/base/hownet_dic/hownet_negative_review.txt', 'gbk'),\n",
    "    ('sentiment_dic/base/hownet_dic/hownet_positive_review.txt', 'gbk'),\n",
    "    ('sentiment_dic/base/hownet_dic/hownet_negative_sentiment.txt', 'gbk'),\n",
    "    ('sentiment_dic/base/hownet_dic/hownet_positive_sentiment.txt', 'gbk')\n",
    "]\n",
    "\n",
    "# Define the weights for each sentiment word file\n",
    "SENTIMENT_WEIGHTS = [\n",
    "    -1, # negative NTUSD\n",
    "    1,  # positive NTUSD\n",
    "    -1, # negative HowNet review\n",
    "    1,  # positive HowNet review\n",
    "    -2, # negative HowNet sentiment\n",
    "    2   # positive HowNet sentiment\n",
    "]\n",
    "\n",
    "# Create a dictionary of sentiment words and their weights\n",
    "sentiment_dict = {}\n",
    "for file_path, encoding in SENTIMENT_FILES:\n",
    "    words = read_txt_file(file_path, encoding)\n",
    "    weight = SENTIMENT_WEIGHTS.pop(0)\n",
    "    for word in words:\n",
    "        sentiment_dict[word] = weight\n",
    "\n",
    "# Save the dictionary as a JSON file\n",
    "with open('sentiment_dic/senti_dic.txt', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sentiment_dict, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a52cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting the stored dictionaries\n",
    "with open('sentiment_dic/senti_dic.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "content = content.replace(' \":', '\":')\n",
    "with open('sentiment_dic/senti_dic.txt', 'w') as file:\n",
    "    file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0860e033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the two txt files and convert them into dictionaries\n",
    "with open('sentiment_dic/senti_dic.txt', 'r', encoding='utf-8') as f:\n",
    "    dict1 = json.load(f)\n",
    "\n",
    "with open('sentiment_dic/neg_dic.txt', 'r', encoding='utf-8') as f:\n",
    "    dict2 = json.load(f)\n",
    "\n",
    "with open('sentiment_dic/adv_dic.txt', 'r', encoding='utf-8') as f:\n",
    "    dict3 = json.load(f)\n",
    "\n",
    "# Remove keys in dict1 that are also in dict2\n",
    "for key in dict2.keys():\n",
    "    if key in dict1:\n",
    "        del dict1[key]\n",
    "        \n",
    "# Remove keys in dict1 that are also in dict3\n",
    "for key in dict3.keys():\n",
    "    if key in dict1:\n",
    "        del dict1[key]\n",
    "\n",
    "# Save the updated dict1 to a new txt file\n",
    "with open('sentiment_dic/senti_dic_pro.txt', 'w', encoding='utf-8') as f:\n",
    "    json.dump(dict1, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0871f4a0",
   "metadata": {},
   "source": [
    "## 2. Calculation of emotional value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30e56d5",
   "metadata": {},
   "source": [
    "### Algorithm design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "248bc12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sentiment dictionary\n",
    "with open('sentiment_dic/senti_dic_pro.txt', 'r', encoding='utf-8') as f:\n",
    "    sentiment_dict = json.load(f)\n",
    "\n",
    "# Read degree adverb dictionary\n",
    "with open('sentiment_dic/adv_dic.txt', 'r', encoding='utf-8') as f:\n",
    "    degree_dict = json.load(f)\n",
    "\n",
    "# Read negation dictionary\n",
    "with open('sentiment_dic/neg_dic.txt', 'r', encoding='utf-8') as f:\n",
    "    negation_dict = json.load(f)\n",
    "\n",
    "# Function to calculate sentiment score for a single sentence\n",
    "def sentiment_score(sen):\n",
    "    seg_list = sen.split()  # Convert text to list of words\n",
    "    emotion_score = 0  # Sentiment score\n",
    "    neg_count = 0  # Count of negation words\n",
    "    for i in range(len(seg_list)):\n",
    "        # Check for sentiment words\n",
    "        if seg_list[i] in sentiment_dict:\n",
    "            score = sentiment_dict[seg_list[i]]\n",
    "            degree_score = 1  # Default degree score is 1\n",
    "            negation_score = 1  # Default negation score is 1\n",
    "            # Look for degree and negation words within the valid distance\n",
    "            for j in range(max(0, i-3), i):\n",
    "                if seg_list[j] in degree_dict:\n",
    "                    degree_score *= degree_dict[seg_list[j]]\n",
    "                elif seg_list[j] in negation_dict:\n",
    "                    neg_count += 1\n",
    "            # Determine whether the sentiment score should be reversed based on the number of negation words\n",
    "            if neg_count % 2 == 0:\n",
    "                emotion_score += score * degree_score\n",
    "            else:\n",
    "                emotion_score -= score * degree_score\n",
    "            neg_count = 0  # Reset negation count\n",
    "    return round(emotion_score, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fdd7249",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.0\n"
     ]
    }
   ],
   "source": [
    "# Testing a typical positive comment\n",
    "sen = '位置 优秀 从来不 担心 车 没 地方 停 收费 贵 点儿 地下 停车场 设计 很 好 停车 极其 楼 基础设施 完善 逛起来 很 舒服 好评 点赞'\n",
    "score = sentiment_score(sen)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cf63f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2\n"
     ]
    }
   ],
   "source": [
    "# Testing a typical negative comment\n",
    "sen = '座位 没有 想 坐下 休息 地方 没有 逛累 顾客 滚蛋'\n",
    "score = sentiment_score(sen)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c3dcbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.02\n"
     ]
    }
   ],
   "source": [
    "# Testing a typical negative comment\n",
    "sen = '商场 整体 环境 还 行 有点 闷 月份 很 热 吃 饭 热 不行'\n",
    "score = sentiment_score(sen)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af606120",
   "metadata": {},
   "source": [
    "### Apply to previously processed comment text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "961c815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process four text files\n",
    "for i in range(1, 5):\n",
    "    # Read in text file\n",
    "    with open(f'../data/review/processed/reviews_{i}.txt', 'r', encoding='utf-8') as f:\n",
    "        reviews = f.readlines()\n",
    "\n",
    "    # Calculate sentiment score and polarity for each review\n",
    "    sentiment_scores = []\n",
    "    polarity = []\n",
    "    for review in reviews:\n",
    "        score = sentiment_score(review)\n",
    "        sentiment_scores.append(score)\n",
    "        if score < 0:\n",
    "            polarity.append('N') # If sentiment score is negative, assign 'N' for negative polarity\n",
    "        elif score >= 1:\n",
    "            polarity.append('P') # If sentiment score is positive, assign 'P' for positive polarity\n",
    "        else:\n",
    "            polarity.append('I') # If sentiment score is neutral, assign 'I' for neutral polarity\n",
    "\n",
    "    # Create a DataFrame for the reviews and their sentiment scores and polarity\n",
    "    df = pd.DataFrame(reviews, columns=['review_text'])\n",
    "    df['sentiment_score'] = sentiment_scores\n",
    "    df['polarity'] = polarity\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(f'../data/review/processed/with_sentiment_score/reviews_{i}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16691c3e",
   "metadata": {},
   "source": [
    "## 3. Use snownlp to help verify comment polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f128964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process four text files\n",
    "for i in range(1, 5):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(f'../data/review/processed/with_sentiment_score/reviews_{i}.csv')\n",
    "\n",
    "    # Read the review text file\n",
    "    with open(f'../data/review/reviews_{i}.txt', 'r', encoding='utf-8') as f:\n",
    "        reviews = f.readlines()\n",
    "\n",
    "    # Calculate sentiment score and polarity for each review and store in lists\n",
    "    sentiment_scores = []\n",
    "    polarities = []\n",
    "    for review in reviews:\n",
    "        s = SnowNLP(review)\n",
    "        sentiment_scores.append(s.sentiments)\n",
    "        if s.sentiments < 0.4:\n",
    "            polarities.append('N')\n",
    "        elif s.sentiments < 0.6:\n",
    "            polarities.append('I')\n",
    "        else:\n",
    "            polarities.append('P')\n",
    "\n",
    "    # Add the sentiment scores and polarities as new columns to the DataFrame\n",
    "    df['snownlp'] = sentiment_scores\n",
    "    df['polarity_snow_nlp'] = polarities\n",
    "\n",
    "    # Write the modified data back to the original CSV file\n",
    "    df.to_csv(f'../data/review/processed/with_sentiment_score/reviews_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c69e350",
   "metadata": {},
   "source": [
    "## 4. Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573fa52b",
   "metadata": {},
   "source": [
    "### Data Concatenation and Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3113c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 5):\n",
    "    # Read in the first CSV file\n",
    "    df_csv_1 = pd.read_csv(f'../data/review/processed/with_sentiment_score/reviews_{i}.csv')\n",
    "    # Read in the second CSV file\n",
    "    df_csv_2 = pd.read_csv(f'../data/review/reviews_{i}.csv')\n",
    "    # Concatenate the two DataFrames horizontally\n",
    "    df_concat = pd.concat([df_csv_1, df_csv_2], axis=1)\n",
    "    # Save the concatenated DataFrame as a CSV file\n",
    "    df_concat.to_csv(f'../data/review/analysis/reviews_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1321eb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store DataFrames\n",
    "dfs = {}\n",
    "\n",
    "# Loop through CSV files\n",
    "for i in range(1, 5):\n",
    "    # Read in a CSV file\n",
    "    df = pd.read_csv(f'../data/review/analysis/reviews_{i}.csv')\n",
    "    \n",
    "    # Get the name of the last column\n",
    "    last_col = df.columns[-1]\n",
    "    \n",
    "    # Move the last column to the beginning of the DataFrame\n",
    "    df = df.iloc[:, [df.shape[1]-1] + list(range(df.shape[1]-1))]\n",
    "    \n",
    "    # Rename the columns for consistency\n",
    "    df = df.rename(columns={\n",
    "        last_col: 'review',\n",
    "        'review_text': 'review_splitting',\n",
    "        'sentiment_score': 'senti_score',\n",
    "        'polarity': 'polarity',\n",
    "        'snownlp': 'senti_score_snownlp',\n",
    "        'polarity_snow_nlp': 'polarity_snownlp'\n",
    "    })\n",
    "    \n",
    "    # Add the DataFrame to the dictionary\n",
    "    dfs[f'reviews_{i}'] = df\n",
    "\n",
    "# Assign DataFrames to variables\n",
    "reviews_1_df = dfs['reviews_1']\n",
    "reviews_2_df = dfs['reviews_2']\n",
    "reviews_3_df = dfs['reviews_3']\n",
    "reviews_4_df = dfs['reviews_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6083d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>review_splitting</th>\n",
       "      <th>senti_score</th>\n",
       "      <th>polarity</th>\n",
       "      <th>senti_score_snownlp</th>\n",
       "      <th>polarity_snownlp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>特别赞的一家商场,在王府井这个商场云集和大牌云集的地方,apm商场算是人气特别高的了,交通,...</td>\n",
       "      <td>特别 赞 一家 商场 王府井 商场 云集 大牌 云集 地方 apm 商场 算是 人气 特别 ...</td>\n",
       "      <td>18.25</td>\n",
       "      <td>P</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LINLEE在王府井apm也开新店了,超级喜欢他们家的口味,和朋友逛街无意看到的,果断去买一...</td>\n",
       "      <td>LINLEE 王府井 apm 开 新店 超级 喜欢 家 口味 朋友 逛街 无意 果断 买 一...</td>\n",
       "      <td>10.30</td>\n",
       "      <td>P</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>北京apm,东城区商场热门榜第一名️,王府井大街号,环境,商场环境干净整齐,布局分明,美食购...</td>\n",
       "      <td>北京 apm 东城区 商场 热门 榜 第一名 ️ 王府井大街 号 环境 商场 环境 干净 整...</td>\n",
       "      <td>4.00</td>\n",
       "      <td>P</td>\n",
       "      <td>0.999969</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>一定要错过下午点半以后,不然吃饭只能排队等半个小时,而且拿到号了一定别因为还有十几桌就走开,...</td>\n",
       "      <td>错过 下午 点半 吃饭 只能 排队 半个 小时 拿到 号 别 十几桌 走开 过 号 好久\\n</td>\n",
       "      <td>0.00</td>\n",
       "      <td>I</td>\n",
       "      <td>0.016099</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>花,Young的年华,年️日,月日,北京apm首层中庭,迎来位艺术家作品联展,春意盎然的️,...</td>\n",
       "      <td>花 Young 年华 年 ️ 日 月 日 北京 apm 首层 中庭 迎来 位 艺术家 作品 ...</td>\n",
       "      <td>7.00</td>\n",
       "      <td>P</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4253</th>\n",
       "      <td>在王府井较大的商场,亮堂宽敞,品牌云集,原来叫新东安商场,周围也云集了很多商场,商场之间有些...</td>\n",
       "      <td>王府井 较大 商场 亮堂 宽敞 品牌 云集 新东安 商场 云集 商场 商场 之间 有些 差异...</td>\n",
       "      <td>-2.30</td>\n",
       "      <td>N</td>\n",
       "      <td>0.999937</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4254</th>\n",
       "      <td>新年伊始,出门看个天使陷落,打卡完小吃,正好到最近的apm百老汇,王府井的老商场了,各种品牌...</td>\n",
       "      <td>新年伊始 出门 看个 天使 陷落 打卡 完 小吃 正好 apm 百老汇 王府井 老 商场 品...</td>\n",
       "      <td>2.60</td>\n",
       "      <td>P</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4255</th>\n",
       "      <td>常去apm不是为逛街,主要为了吃,当然吃完也要逛逛消食,北京apm,新东安广场,地处北京王府...</td>\n",
       "      <td>常去 apm 不是 逛街 吃 吃 完 逛逛 消食 北京 apm 新东安 广场 地处 北京王府...</td>\n",
       "      <td>6.20</td>\n",
       "      <td>P</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256</th>\n",
       "      <td>王府井apm经常和媳妇儿过来逛因为有的吃有的逛而且媳妇儿喜欢这儿的外婆家这次过来是陪朋友买手...</td>\n",
       "      <td>王府井 apm 媳妇儿 逛 吃 逛 媳妇儿 喜欢 外婆家 陪 朋友 买手机 顺便 逛逛 媳妇...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>P</td>\n",
       "      <td>0.977859</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4257</th>\n",
       "      <td>北京王府井商业区里的新成员,商场里入住了很多店铺,有卖衣服的,鞋子的,首饰的,当然也少不了很...</td>\n",
       "      <td>北京王府井 商业区 里 新 成员 商场 里 入住 店铺 卖 衣服 鞋子 首饰 少不了 很多很...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>P</td>\n",
       "      <td>0.197848</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4258 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  \\\n",
       "0     特别赞的一家商场,在王府井这个商场云集和大牌云集的地方,apm商场算是人气特别高的了,交通,...   \n",
       "1     LINLEE在王府井apm也开新店了,超级喜欢他们家的口味,和朋友逛街无意看到的,果断去买一...   \n",
       "2     北京apm,东城区商场热门榜第一名️,王府井大街号,环境,商场环境干净整齐,布局分明,美食购...   \n",
       "3     一定要错过下午点半以后,不然吃饭只能排队等半个小时,而且拿到号了一定别因为还有十几桌就走开,...   \n",
       "4     花,Young的年华,年️日,月日,北京apm首层中庭,迎来位艺术家作品联展,春意盎然的️,...   \n",
       "...                                                 ...   \n",
       "4253  在王府井较大的商场,亮堂宽敞,品牌云集,原来叫新东安商场,周围也云集了很多商场,商场之间有些...   \n",
       "4254  新年伊始,出门看个天使陷落,打卡完小吃,正好到最近的apm百老汇,王府井的老商场了,各种品牌...   \n",
       "4255  常去apm不是为逛街,主要为了吃,当然吃完也要逛逛消食,北京apm,新东安广场,地处北京王府...   \n",
       "4256  王府井apm经常和媳妇儿过来逛因为有的吃有的逛而且媳妇儿喜欢这儿的外婆家这次过来是陪朋友买手...   \n",
       "4257  北京王府井商业区里的新成员,商场里入住了很多店铺,有卖衣服的,鞋子的,首饰的,当然也少不了很...   \n",
       "\n",
       "                                       review_splitting  senti_score polarity  \\\n",
       "0     特别 赞 一家 商场 王府井 商场 云集 大牌 云集 地方 apm 商场 算是 人气 特别 ...        18.25        P   \n",
       "1     LINLEE 王府井 apm 开 新店 超级 喜欢 家 口味 朋友 逛街 无意 果断 买 一...        10.30        P   \n",
       "2     北京 apm 东城区 商场 热门 榜 第一名 ️ 王府井大街 号 环境 商场 环境 干净 整...         4.00        P   \n",
       "3        错过 下午 点半 吃饭 只能 排队 半个 小时 拿到 号 别 十几桌 走开 过 号 好久\\n         0.00        I   \n",
       "4     花 Young 年华 年 ️ 日 月 日 北京 apm 首层 中庭 迎来 位 艺术家 作品 ...         7.00        P   \n",
       "...                                                 ...          ...      ...   \n",
       "4253  王府井 较大 商场 亮堂 宽敞 品牌 云集 新东安 商场 云集 商场 商场 之间 有些 差异...        -2.30        N   \n",
       "4254  新年伊始 出门 看个 天使 陷落 打卡 完 小吃 正好 apm 百老汇 王府井 老 商场 品...         2.60        P   \n",
       "4255  常去 apm 不是 逛街 吃 吃 完 逛逛 消食 北京 apm 新东安 广场 地处 北京王府...         6.20        P   \n",
       "4256  王府井 apm 媳妇儿 逛 吃 逛 媳妇儿 喜欢 外婆家 陪 朋友 买手机 顺便 逛逛 媳妇...         1.00        P   \n",
       "4257  北京王府井 商业区 里 新 成员 商场 里 入住 店铺 卖 衣服 鞋子 首饰 少不了 很多很...         1.00        P   \n",
       "\n",
       "      senti_score_snownlp polarity_snownlp  \n",
       "0                1.000000                P  \n",
       "1                1.000000                P  \n",
       "2                0.999969                P  \n",
       "3                0.016099                N  \n",
       "4                0.999991                P  \n",
       "...                   ...              ...  \n",
       "4253             0.999937                P  \n",
       "4254             0.999997                P  \n",
       "4255             0.999995                P  \n",
       "4256             0.977859                P  \n",
       "4257             0.197848                N  \n",
       "\n",
       "[4258 rows x 6 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0270691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
