{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61b67bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import regex as re\n",
    "import string\n",
    "import csv\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574b7fe6",
   "metadata": {},
   "source": [
    "## 1. Extract the review data from csv and save it as txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29a4e4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file paths to read from\n",
    "file_paths = ['../data/北京apm.csv', '../data/王府中環.csv', '../data/王府井百货.csv', '../data/东方新天地.csv']\n",
    "\n",
    "# Loop over each file, extract the data from the 'review' column and save it to a text file\n",
    "for i, path in enumerate(file_paths):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(path, index_col=0)  # Set the index_col to 0 to use the first column as the index\n",
    "    # Define the path for the output text file\n",
    "    txt_path = f'../data/review/reviews_{i+1}.txt'\n",
    "    # Write the review data to the output text file\n",
    "    with open(txt_path, 'w') as f:\n",
    "        for j, review in enumerate(df['review']):\n",
    "            # Clean up the text data by removing special characters\n",
    "            review = re.sub('[\\n\\r\\t ]+', '', review)\n",
    "            # Write the review data to the output text file\n",
    "            f.write(str(review) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bb27d7",
   "metadata": {},
   "source": [
    "## 2. Formatting (Remove emoji, spaces & Replace punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32be6a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file paths to modify\n",
    "txt_paths = ['../data/review/reviews_1.txt', '../data/review/reviews_2.txt', '../data/review/reviews_3.txt', '../data/review/reviews_4.txt']\n",
    "\n",
    "# Define a string of all punctuation marks\n",
    "all_punct = string.punctuation + '，。？！：；（）“”‘’【】『』《》〈〉·—…～、｜「」'\n",
    "\n",
    "# Define regular expression matching pattern\n",
    "pattern = re.compile(',+')\n",
    "\n",
    "# Loop over each file and modify its contents\n",
    "for txt_path in txt_paths:\n",
    "    # Open the file for reading\n",
    "    with open(txt_path, 'r') as file:\n",
    "        # Read the contents of the file and replace all punctuation marks with commas\n",
    "        contents = file.read().translate(str.maketrans(all_punct, ',' * len(all_punct)))\n",
    "        \n",
    "        # Remove all emoji symbols\n",
    "        contents = re.sub(r'\\p{Emoji}', '', contents)\n",
    "        \n",
    "        # Replacing consecutive commas\n",
    "        contents = re.sub(pattern, ',', contents)\n",
    "        \n",
    "        # Remove trailing commas from each line\n",
    "        contents = '\\n'.join(line.rstrip(',') for line in contents.split('\\n'))\n",
    "        \n",
    "    # Open the file for writing\n",
    "    with open(txt_path, 'w') as file:\n",
    "        # Write the modified contents to the file\n",
    "        file.write(contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bda8013",
   "metadata": {},
   "source": [
    "## 3. Merge into a new txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ee4815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file paths to read from\n",
    "txt_paths = ['../data/review/reviews_1.txt', '../data/review/reviews_2.txt', '../data/review/reviews_3.txt', '../data/review/reviews_4.txt']\n",
    "\n",
    "# Define the path for the output merged text file\n",
    "merged_txt_path = '../data/review/reviews_merged.txt'\n",
    "\n",
    "# Open the output file for writing\n",
    "with open(merged_txt_path, 'w', newline='\\n') as merged_file:\n",
    "    # Loop over each input file\n",
    "    for txt_path in txt_paths:\n",
    "        # Open the input file for reading\n",
    "        with open(txt_path, 'r', newline='') as input_file:\n",
    "            # Loop over each line in the input file\n",
    "            for line in input_file:\n",
    "                # Write the line to the output file\n",
    "                merged_file.write(line.rstrip('\\r\\n') + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1600e4ba",
   "metadata": {},
   "source": [
    "### Read into dataframe to validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac15f508",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      特别赞的一家商场,在王府井这个商场云集和大牌云集的地方,apm商场算是人气特别高的了,交通,地铁号线金鱼胡同地铁口,交通便利,环境,整个商场特别收拾的很干净而且吃喝完了种类特别的全,这次来正好看到有画展,很有意思还在画展逛了一圈的,很不错,就逛逛别的地方,这里正的是可以让你带一天的好地方\n",
       "1    LINLEE在王府井apm也开新店了,超级喜欢他们家的口味,和朋友逛街无意看到的,果断去买一杯比较推荐他家的泰绿手打柠檬茶和手打柠檬茶,如果喜欢喝酸一点的是特浓手打柠檬茶,个人推荐三分甜,感觉酸涩度刚刚的好,夏天了,喝柠檬茶解渴而且还补充维C,来一杯,元气满满,超赞而且还有小鸭子送,太可爱了\n",
       "2                                        北京apm,东城区商场热门榜第一名️,王府井大街号,环境,商场环境干净整齐,布局分明,美食购物️丽人,汽车为一体,坐落于王府井大街,每一层都有每一层的特色,其中就有必吃的小大董,还有捞王锅物料理,楼全是美食,麻六记,局气\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to the merged text file\n",
    "merged_txt_path = '../data/review/reviews_merged.txt'\n",
    "\n",
    "# Read the merged text file into a DataFrame\n",
    "df = pd.read_csv(merged_txt_path, delimiter='\\t', names=['review'])\n",
    "\n",
    "# Print the DataFrame\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df['review'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b35749",
   "metadata": {},
   "source": [
    "## 4. Convert to csv file for storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fd2bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of txt file paths that need to be converted\n",
    "txt_paths = ['../data/review/reviews_1.txt', '../data/review/reviews_2.txt', '../data/review/reviews_3.txt', '../data/review/reviews_4.txt', '../data/review/reviews_merged.txt']\n",
    "\n",
    "# Loop through each txt file and convert it to a csv file\n",
    "for txt_path in txt_paths:\n",
    "    # Generate the corresponding csv file path\n",
    "    csv_path = txt_path.replace('.txt', '.csv')\n",
    "    \n",
    "    # Open the txt file and csv file\n",
    "    with open(txt_path, 'r', encoding='utf-8') as f_txt, open(csv_path, 'w', encoding='utf-8', newline='') as f_csv:\n",
    "        # Create a csv.writer object to write data to the csv file\n",
    "        writer = csv.writer(f_csv)\n",
    "        \n",
    "        # Write the header row\n",
    "        writer.writerow(['review'])\n",
    "        \n",
    "        # Read each line in the txt file and write it to the csv file\n",
    "        for line in f_txt:\n",
    "            # Wrap each line of data in a list to represent a row of data in the csv file\n",
    "            writer.writerow([line.strip()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866bbc85",
   "metadata": {},
   "source": [
    "## 5. Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ce4203",
   "metadata": {},
   "source": [
    "### Custom stopwords dictionary（two different versions）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96cb12db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of file paths for the stop word files\n",
    "stopword_paths = ['stopwords/source/hit_stopwords.txt', 'stopwords/source/scu_stopwords.txt', 'stopwords/source/baidu_stopwords.txt', 'stopwords/source/cn_stopwords.txt']\n",
    "\n",
    "# Read all stop words into a single list\n",
    "stopwords = []\n",
    "for path in stopword_paths:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        # Read each line in the file, strip whitespace, and add the resulting words to the list\n",
    "        words = [w.strip() for w in f.readlines()]\n",
    "        stopwords.extend(words)\n",
    "\n",
    "# Remove duplicate stop words\n",
    "stopwords = list(set(stopwords))\n",
    "\n",
    "# Sort the stop words alphabetically\n",
    "stopwords.sort()\n",
    "\n",
    "# Write all stop words to a file\n",
    "with open('stopwords/stopwords.txt', 'w', encoding='utf-8') as f:\n",
    "    # Join the stop words into a single string separated by newlines, and write to the file\n",
    "    f.write('\\n'.join(stopwords))\n",
    "\n",
    "# Read adverb words from file\n",
    "adv_path = 'sentiment_dic/base/adv_dic.txt'\n",
    "with open(adv_path, 'r', encoding='utf-8') as f:\n",
    "    adverbs = [w.strip() for w in f.readlines()]\n",
    "\n",
    "# Remove adverb words from stopwords\n",
    "stopwords = [w for w in stopwords if w not in adverbs]\n",
    "\n",
    "# Read negative words from file\n",
    "neg_path = 'sentiment_dic/base/negative_dic.txt'\n",
    "with open(neg_path, 'r', encoding='utf-8') as f:\n",
    "    negatives = [w.strip() for w in f.readlines()]\n",
    "\n",
    "# Remove negative words from stopwords\n",
    "stopwords = [w for w in stopwords if w not in negatives]\n",
    "\n",
    "# Sort the stop words alphabetically\n",
    "stopwords.sort()\n",
    "\n",
    "# Write modified stop words to a file\n",
    "with open('stopwords/stopwords_pro.txt', 'w', encoding='utf-8') as f:\n",
    "    # Join the stop words into a single string separated by newlines, and write to the file\n",
    "    f.write('\\n'.join(stopwords))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dc630d",
   "metadata": {},
   "source": [
    "### Split word processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "070f73dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/st/jr2134cd02575p7pnq8xy5440000gn/T/jieba.cache\n",
      "Loading model cost 0.283 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the file paths to read from and write to\n",
    "input_file_paths = ['../data/review/reviews_1.txt', '../data/review/reviews_2.txt', '../data/review/reviews_3.txt', '../data/review/reviews_4.txt', '../data/review/reviews_merged.txt']\n",
    "output_directory = '../data/review/processed/'\n",
    "\n",
    "# Set the file path for the stop words\n",
    "stopword_path = 'stopwords/stopwords_pro.txt'\n",
    "\n",
    "# Create a set of stop words by opening the file and using a set comprehension\n",
    "with open(stopword_path, 'r', encoding='utf-8') as f:\n",
    "    stopwords = {word.strip() for word in f}\n",
    "\n",
    "# Load the custom dictionary\n",
    "custom_dict_path = 'stopwords/custom_dict.txt'\n",
    "jieba.load_userdict(custom_dict_path)\n",
    "\n",
    "# Define a function to process each review text\n",
    "def process_text(text):\n",
    "    # Tokenize the text with Jieba and enable HMM\n",
    "    tokens = jieba.cut(text, cut_all=False, HMM=True)\n",
    "    # Remove stop words and punctuation\n",
    "    tokens = [token for token in tokens if token not in stopwords and token.strip() and not token.isnumeric()]\n",
    "    # Return the processed text as a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Loop over each file, process the review text and save it to a new text file\n",
    "for input_file_path in input_file_paths:\n",
    "    # Create the output file path\n",
    "    output_file_path = os.path.join(output_directory, os.path.basename(input_file_path))\n",
    "    # Open the input and output files\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as input_file, open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        # Process each line of the input file\n",
    "        for line in input_file:\n",
    "            # Process the review text\n",
    "            processed_text = process_text(line.strip())\n",
    "            # Write the processed text to the output file\n",
    "            output_file.write(processed_text + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c6285",
   "metadata": {},
   "source": [
    "## 6.Part-of-speech tagging and word frequency statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4158f0",
   "metadata": {},
   "source": [
    "### 北京APM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea17158e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "商场 3607\n",
      "王府井 1894\n",
      "逛 1654\n",
      "品牌 1290\n",
      "吃 1189\n",
      "北京 1137\n",
      "店 1125\n",
      "买 1004\n",
      "喜欢 1001\n",
      "活动 982\n",
      "没有 775\n",
      "感觉 766\n",
      "没 636\n",
      "吃饭 604\n",
      "逛街 567\n",
      "疫情 540\n",
      "地方 511\n",
      "环境 478\n",
      "购物 467\n",
      "打卡 465\n",
      "逛逛 454\n",
      "开 422\n",
      "餐饮 422\n",
      "拍照 421\n",
      "朋友 407\n",
      "店铺 400\n",
      "时尚 395\n",
      "美食 391\n",
      "想 391\n",
      "适合 388\n",
      "东西 387\n",
      "东安市场 382\n",
      "说 378\n",
      "步行街 371\n",
      "衣服 366\n",
      "装修 351\n",
      "排队 349\n",
      "苹果 348\n",
      "展览 347\n",
      "王府井大街 331\n",
      "玩 331\n",
      "做 326\n",
      "爱 320\n",
      "人 313\n",
      "餐厅 311\n",
      "可爱 305\n",
      "大牌 300\n",
      "体验 290\n",
      "整体 286\n",
      "位置 286\n"
     ]
    }
   ],
   "source": [
    "# Read file\n",
    "with open('../data/review/processed/reviews_1.txt', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Perform Part-of-Speech tagging on the text\n",
    "words = pseg.cut(content)\n",
    "\n",
    "# Count the frequency of nouns and verbs\n",
    "word_counts = Counter([word for word, flag in words if flag.startswith('n') or flag.startswith('v')])\n",
    "\n",
    "# Print the 50 most frequently occurring words\n",
    "for word, count in word_counts.most_common(50):\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0115dab7",
   "metadata": {},
   "source": [
    "### 王府中環"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "224e26cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "商场 2240\n",
      "王府井 1050\n",
      "品牌 827\n",
      "环境 826\n",
      "咖啡 764\n",
      "喜欢 758\n",
      "逛 746\n",
      "吃 583\n",
      "北京 572\n",
      "没有 548\n",
      "感觉 544\n",
      "餐厅 518\n",
      "高端 501\n",
      "地方 485\n",
      "买 484\n",
      "大牌 481\n",
      "店 453\n",
      "没 403\n",
      "人 374\n",
      "活动 370\n",
      "说 367\n",
      "打卡 354\n",
      "购物 351\n",
      "拍照 335\n",
      "吃饭 328\n",
      "带 325\n",
      "适合 311\n",
      "卫生间 280\n",
      "孩子 280\n",
      "停车场 271\n",
      "中环 269\n",
      "疫情 264\n",
      "餐饮 259\n",
      "服务 252\n",
      "开 247\n",
      "整体 246\n",
      "东西 244\n",
      "草坪 239\n",
      "爱 237\n",
      "喝 227\n",
      "娃 227\n",
      "奢侈品 224\n",
      "停车 223\n",
      "步行街 220\n",
      "想 220\n",
      "设计 219\n",
      "排队 217\n",
      "朋友 215\n",
      "拍 210\n",
      "体验 208\n"
     ]
    }
   ],
   "source": [
    "# Read file\n",
    "with open('../data/review/processed/reviews_2.txt', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Perform Part-of-Speech tagging on the text\n",
    "words = pseg.cut(content)\n",
    "\n",
    "# Count the frequency of nouns and verbs\n",
    "word_counts = Counter([word for word, flag in words if flag.startswith('n') or flag.startswith('v')])\n",
    "\n",
    "# Print the 50 most frequently occurring words\n",
    "for word, count in word_counts.most_common(50):\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bec344",
   "metadata": {},
   "source": [
    "### 王府井百货"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1122a310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "百货大楼 1609\n",
      "北京 1228\n",
      "王府井 1019\n",
      "商场 702\n",
      "和平 423\n",
      "逛 412\n",
      "买 384\n",
      "没有 373\n",
      "感觉 357\n",
      "地方 294\n",
      "没 287\n",
      "小时候 285\n",
      "品牌 250\n",
      "回忆 242\n",
      "北京市 235\n",
      "局 233\n",
      "打卡 230\n",
      "步行街 221\n",
      "王府井大街 217\n",
      "说 210\n",
      "张秉贵 204\n",
      "吃 201\n",
      "东西 194\n",
      "胡同 191\n",
      "喜欢 183\n",
      "店 181\n",
      "带 177\n",
      "疫情 174\n",
      "逛逛 171\n",
      "中国 169\n",
      "记忆 162\n",
      "孩子 161\n",
      "化妆品 160\n",
      "人 158\n",
      "购物 158\n",
      "果局 152\n",
      "卖 152\n",
      "环境 152\n",
      "玩具 143\n",
      "想 136\n",
      "来 135\n",
      "活动 135\n",
      "大楼 133\n",
      "拍照 124\n",
      "一楼 115\n",
      "位于 115\n",
      "感 114\n",
      "朋友 114\n",
      "值得 113\n",
      "游客 113\n"
     ]
    }
   ],
   "source": [
    "# Read file\n",
    "with open('../data/review/processed/reviews_3.txt', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Perform Part-of-Speech tagging on the text\n",
    "words = pseg.cut(content)\n",
    "\n",
    "# Count the frequency of nouns and verbs\n",
    "word_counts = Counter([word for word, flag in words if flag.startswith('n') or flag.startswith('v')])\n",
    "\n",
    "# Print the 50 most frequently occurring words\n",
    "for word, count in word_counts.most_common(50):\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5b50b8",
   "metadata": {},
   "source": [
    "### 东方新天地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35c3575f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "商场 970\n",
      "王府井 513\n",
      "品牌 379\n",
      "逛 348\n",
      "吃 240\n",
      "购物 232\n",
      "没有 231\n",
      "环境 217\n",
      "疫情 205\n",
      "北京 202\n",
      "东单 201\n",
      "地方 196\n",
      "地铁 196\n",
      "店 196\n",
      "喜欢 194\n",
      "没 193\n",
      "感觉 191\n",
      "吃饭 169\n",
      "买 165\n",
      "停车 162\n",
      "店铺 158\n",
      "走 140\n",
      "长安街 136\n",
      "餐饮 134\n",
      "开 126\n",
      "逛街 123\n",
      "说 122\n",
      "东方广场 118\n",
      "东西 117\n",
      "新天地 112\n",
      "服务 107\n",
      "大牌 106\n",
      "想 102\n",
      "位置 102\n",
      "交通 98\n",
      "人 96\n",
      "活动 91\n",
      "高端 91\n",
      "逛逛 89\n",
      "免费 88\n",
      "美食 87\n",
      "适合 86\n",
      "选择 85\n",
      "时间 85\n",
      "来 84\n",
      "找 80\n",
      "朋友 78\n",
      "餐厅 78\n",
      "位于 78\n",
      "步行街 74\n"
     ]
    }
   ],
   "source": [
    "# Read file\n",
    "with open('../data/review/processed/reviews_4.txt', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Perform Part-of-Speech tagging on the text\n",
    "words = pseg.cut(content)\n",
    "\n",
    "# Count the frequency of nouns and verbs\n",
    "word_counts = Counter([word for word, flag in words if flag.startswith('n') or flag.startswith('v')])\n",
    "\n",
    "# Print the 50 most frequently occurring words\n",
    "for word, count in word_counts.most_common(50):\n",
    "    print(word, count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
