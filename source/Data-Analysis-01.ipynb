{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cbe1bba",
   "metadata": {},
   "source": [
    "# Part 1. Calculation and Analysis of Sentiment Value of Review Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "06ec2db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import jieba\n",
    "import pandas as pd\n",
    "from snownlp import SnowNLP\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed265f6",
   "metadata": {},
   "source": [
    "## 1. Building an sentiment lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f94efa",
   "metadata": {},
   "source": [
    "### Dictionary for degree adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc2e6d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define weights for different degree adverbs\n",
    "category1 = {\n",
    "    '绝对': 2,\n",
    "    '绝对化': 2,\n",
    "    '绝对性': 2,\n",
    "    '绝顶': 2,\n",
    "    '万分': 2,\n",
    "    '不得了': 2,\n",
    "    '不可开交': 2,\n",
    "    '不亦乐乎': 2,\n",
    "    '不折不扣': 2,\n",
    "    '彻头彻尾': 2,\n",
    "    '充分': 2,\n",
    "    '到头': 2,\n",
    "    '地地道道': 2,\n",
    "    '非常': 2,\n",
    "    '极': 2,\n",
    "    '极度': 2,\n",
    "    '极端': 2,\n",
    "    '极其': 2,\n",
    "    '极为': 2,\n",
    "    '截然': 2,\n",
    "    '尽': 2,\n",
    "    '惊人地': 2,\n",
    "    '绝': 2,\n",
    "    '刻骨': 2,\n",
    "    '酷': 2,\n",
    "    '满': 2,\n",
    "    '满贯': 2,\n",
    "    '满心': 2,\n",
    "    '莫大': 2,\n",
    "    '奇': 2,\n",
    "    '入骨': 2,\n",
    "    '甚为': 2,\n",
    "    '十二分': 2,\n",
    "    '十分': 2,\n",
    "    '十足': 2,\n",
    "    '死': 2,\n",
    "    '滔天': 2,\n",
    "    '痛': 2,\n",
    "    '透': 2,\n",
    "    '完全': 2,\n",
    "    '完完全全': 2,\n",
    "    '万': 2,\n",
    "    '万般': 2,\n",
    "    '无比': 2,\n",
    "    '无度': 2,\n",
    "    '无可估量': 2,\n",
    "    '无以复加': 2,\n",
    "    '无以伦比': 2,\n",
    "    '要命': 2,\n",
    "    '要死': 2,\n",
    "    '已极': 2,\n",
    "    '已甚': 2,\n",
    "    '异常': 2,\n",
    "    '逾常': 2,\n",
    "    '贼': 2,\n",
    "    '之极': 2,\n",
    "    '之至': 2,\n",
    "    '至极': 2,\n",
    "    '卓绝': 2,\n",
    "    '最为': 2,\n",
    "    '佼佼': 2,\n",
    "    '郅': 2,\n",
    "    '綦': 2,\n",
    "    '齁': 2,\n",
    "    '最': 2,\n",
    "}\n",
    "\n",
    "category2 = {\n",
    "    '不为过': 1.8,\n",
    "    '超': 1.8,\n",
    "    '超额': 1.8,\n",
    "    '超外差': 1.8,\n",
    "    '超微结构': 1.8,\n",
    "    '超物质': 1.8,\n",
    "    '出头': 1.8,\n",
    "    '多': 1.8,\n",
    "    '浮': 1.8,\n",
    "    '过': 1.8,\n",
    "    '过度': 1.8,\n",
    "    '过分': 1.8,\n",
    "    '过火': 1.8,\n",
    "    '过劲': 1.8,\n",
    "    '过了头': 1.8,\n",
    "    '过猛': 1.8,\n",
    "    '过热': 1.8,\n",
    "    '过甚': 1.8,\n",
    "    '过头': 1.8,\n",
    "    '过于': 1.8,\n",
    "    '过逾': 1.8,\n",
    "    '何止': 1.8,\n",
    "    '何啻': 1.8,\n",
    "    '开外': 1.8,\n",
    "    '苦': 1.8,\n",
    "    '老': 1.8,\n",
    "    '偏': 1.8,\n",
    "    '强': 1.8,\n",
    "    '溢': 1.8,\n",
    "    '忒': 1.8\n",
    "}\n",
    "\n",
    "category3 = {\n",
    "    \"不过\": 1.5,\n",
    "    \"不少\": 1.5,\n",
    "    \"不胜\": 1.5,\n",
    "    \"惨\": 1.5,\n",
    "    \"沉\": 1.5,\n",
    "    \"沉沉\": 1.5,\n",
    "    \"出奇\": 1.5,\n",
    "    \"大为\": 1.5,\n",
    "    \"多\": 1.5,\n",
    "    \"多多\": 1.5,\n",
    "    \"多加\": 1.5,\n",
    "    \"多么\": 1.5,\n",
    "    \"分外\": 1.5,\n",
    "    \"格外\": 1.5,\n",
    "    \"够瞧的\": 1.5,\n",
    "    \"够戗\": 1.5,\n",
    "    \"好不\": 1.5,\n",
    "    \"何等\": 1.5,\n",
    "    \"很\": 1.5,\n",
    "    \"很是\": 1.5,\n",
    "    \"坏\": 1.5,\n",
    "    \"可\": 1.5,\n",
    "    \"老\": 1.5,\n",
    "    \"老大\": 1.5,\n",
    "    \"良\": 1.5,\n",
    "    \"颇\": 1.5,\n",
    "    \"颇为\": 1.5,\n",
    "    \"甚\": 1.5,\n",
    "    \"实在\": 1.5,\n",
    "    \"太\": 1.5,\n",
    "    \"太甚\": 1.5,\n",
    "    \"特\": 1.5,\n",
    "    \"特别\": 1.5,\n",
    "    \"尤\": 1.5,\n",
    "    \"尤其\": 1.5,\n",
    "    \"尤为\": 1.5,\n",
    "    \"尤以\": 1.5,\n",
    "    \"远\": 1.5,\n",
    "    \"着实\": 1.5,\n",
    "    \"曷\": 1.5,\n",
    "    \"碜\": 1.5\n",
    "}\n",
    "\n",
    "category4 = {\n",
    "    '大不了': 1.2,\n",
    "    '多': 1.2,\n",
    "    '更': 1.2,\n",
    "    '更加': 1.2,\n",
    "    '更进一步': 1.2,\n",
    "    '更为': 1.2,\n",
    "    '还': 1.2,\n",
    "    '还要': 1.2,\n",
    "    '较': 1.2,\n",
    "    '较比': 1.2,\n",
    "    '较为': 1.2,\n",
    "    '进一步': 1.2,\n",
    "    '那般': 1.2,\n",
    "    '那么': 1.2,\n",
    "    '那样': 1.2,\n",
    "    '强': 1.2,\n",
    "    '如斯': 1.2,\n",
    "    '益': 1.2,\n",
    "    '益发': 1.2,\n",
    "    '尤甚': 1.2,\n",
    "    '逾': 1.2,\n",
    "    '愈': 1.2,\n",
    "    '愈 ... 愈': 1.2,\n",
    "    '愈发': 1.2,\n",
    "    '愈加': 1.2,\n",
    "    '愈来愈': 1.2,\n",
    "    '愈益': 1.2,\n",
    "    '远远': 1.2,\n",
    "    '越 ... 越': 1.2,\n",
    "    '越发': 1.2,\n",
    "    '越加': 1.2,\n",
    "    '越来越': 1.2,\n",
    "    '越是': 1.2,\n",
    "    '这般': 1.2,\n",
    "    '这样': 1.2,\n",
    "    '足': 1.2,\n",
    "    '足足': 1.2,\n",
    "}\n",
    "\n",
    "category5 = {\n",
    "    '点点滴滴': 0.8,\n",
    "    '多多少少': 0.8,\n",
    "    '怪': 0.8,\n",
    "    '好生': 0.8,\n",
    "    '还': 0.8,\n",
    "    '或多或少': 0.8,\n",
    "    '略': 0.8,\n",
    "    '略加': 0.8,\n",
    "    '略略': 0.8,\n",
    "    '略微': 0.8,\n",
    "    '略为': 0.8,\n",
    "    '稍': 0.8,\n",
    "    '稍稍': 0.8,\n",
    "    '稍微': 0.8,\n",
    "    '稍为': 0.8,\n",
    "    '稍许': 0.8,\n",
    "    '挺': 0.8,\n",
    "    '未免': 0.8,\n",
    "    '相当': 0.8,\n",
    "    '些': 0.8,\n",
    "    '些微': 0.8,\n",
    "    '些小': 0.8,\n",
    "    '一点': 0.8,\n",
    "    '一点儿': 0.8,\n",
    "    '一些': 0.8,\n",
    "    '有点': 0.8,\n",
    "    '有点儿': 0.8,\n",
    "    '有些': 0.8\n",
    "}\n",
    "\n",
    "category6 = {\n",
    "    \"半点\": 0.5,\n",
    "    \"不大\": 0.5,\n",
    "    \"不丁点儿\": 0.5,\n",
    "    \"不甚\": 0.5,\n",
    "    \"不怎么\": 0.5,\n",
    "    \"聊\": 0.5,\n",
    "    \"没怎么\": 0.5,\n",
    "    \"轻度\": 0.5,\n",
    "    \"弱\": 0.5,\n",
    "    \"丝毫\": 0.5,\n",
    "    \"微\": 0.5,\n",
    "    \"相对\": 0.5\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7985e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of 6 dictionaries\n",
    "dict_list = [category1, category2, category3, category4, category5, category6]\n",
    "\n",
    "# Merge all dictionaries into a single dictionary\n",
    "merged_dict = {}\n",
    "for d in dict_list:\n",
    "    merged_dict.update(d)\n",
    "\n",
    "# Save the merged dictionary to a txt file\n",
    "# Open the file in write mode, encode with utf-8 and write as a json object\n",
    "with open('sentiment_dic/adv_dic.txt', 'w', encoding='utf-8') as f:\n",
    "    json.dump(merged_dict, f, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1539f270",
   "metadata": {},
   "source": [
    "### Dictionary for negative words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac12c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read a txt file and return its contents as a list of lines\n",
    "def read_txt_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read().splitlines()\n",
    "\n",
    "# Read words from negative_dic.txt file\n",
    "neg_words = read_txt_file('sentiment_dic/base/negative_dic.txt')\n",
    "\n",
    "# Convert the list of words into a dictionary and set the value of each word as 1\n",
    "neg_dict = {word: 1 for word in neg_words}\n",
    "\n",
    "# Save the negative dictionary to a txt file\n",
    "# Open the file in write mode, encode with utf-8 and write as a json object\n",
    "with open('sentiment_dic/neg_dic.txt', 'w', encoding='utf-8') as f:\n",
    "    json.dump(neg_dict, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b5bc1",
   "metadata": {},
   "source": [
    "### Dictionary for emotional words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9226945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt_file(file_path, encoding):\n",
    "    with open(file_path, 'r', encoding=encoding) as f:\n",
    "        return f.read().splitlines()\n",
    "\n",
    "# Define the file paths and encodings for each sentiment word file\n",
    "SENTIMENT_FILES = [\n",
    "    ('sentiment_dic/base/ntusd_dic/NTUSD_negative.txt', 'utf-16le'),\n",
    "    ('sentiment_dic/base/ntusd_dic/NTUSD_positive.txt', 'utf-16le'),\n",
    "    ('sentiment_dic/base/hownet_dic/hownet_negative_review.txt', 'gbk'),\n",
    "    ('sentiment_dic/base/hownet_dic/hownet_positive_review.txt', 'gbk'),\n",
    "    ('sentiment_dic/base/hownet_dic/hownet_negative_sentiment.txt', 'gbk'),\n",
    "    ('sentiment_dic/base/hownet_dic/hownet_positive_sentiment.txt', 'gbk')\n",
    "]\n",
    "\n",
    "# Define the weights for each sentiment word file\n",
    "SENTIMENT_WEIGHTS = [\n",
    "    -1, # negative NTUSD\n",
    "    1,  # positive NTUSD\n",
    "    -1, # negative HowNet review\n",
    "    1,  # positive HowNet review\n",
    "    -2, # negative HowNet sentiment\n",
    "    2   # positive HowNet sentiment\n",
    "]\n",
    "\n",
    "# Create a dictionary of sentiment words and their weights\n",
    "sentiment_dict = {}\n",
    "for file_path, encoding in SENTIMENT_FILES:\n",
    "    words = read_txt_file(file_path, encoding)\n",
    "    weight = SENTIMENT_WEIGHTS.pop(0)\n",
    "    for word in words:\n",
    "        sentiment_dict[word] = weight\n",
    "\n",
    "# Save the dictionary as a JSON file\n",
    "with open('sentiment_dic/senti_dic.txt', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sentiment_dict, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a52cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting the stored dictionaries\n",
    "with open('sentiment_dic/senti_dic.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "content = content.replace(' \":', '\":')\n",
    "with open('sentiment_dic/senti_dic.txt', 'w') as file:\n",
    "    file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0860e033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the two txt files and convert them into dictionaries\n",
    "with open('sentiment_dic/senti_dic.txt', 'r', encoding='utf-8') as f:\n",
    "    dict1 = json.load(f)\n",
    "\n",
    "with open('sentiment_dic/neg_dic.txt', 'r', encoding='utf-8') as f:\n",
    "    dict2 = json.load(f)\n",
    "\n",
    "with open('sentiment_dic/adv_dic.txt', 'r', encoding='utf-8') as f:\n",
    "    dict3 = json.load(f)\n",
    "\n",
    "# Remove keys in dict1 that are also in dict2\n",
    "for key in dict2.keys():\n",
    "    if key in dict1:\n",
    "        del dict1[key]\n",
    "        \n",
    "# Remove keys in dict1 that are also in dict3\n",
    "for key in dict3.keys():\n",
    "    if key in dict1:\n",
    "        del dict1[key]\n",
    "\n",
    "# Save the updated dict1 to a new txt file\n",
    "with open('sentiment_dic/senti_dic_pro.txt', 'w', encoding='utf-8') as f:\n",
    "    json.dump(dict1, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0871f4a0",
   "metadata": {},
   "source": [
    "## 2. Calculation of sentiment value score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30e56d5",
   "metadata": {},
   "source": [
    "### Algorithm design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "248bc12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sentiment dictionary\n",
    "with open('sentiment_dic/senti_dic_pro.txt', 'r', encoding='utf-8') as f:\n",
    "    sentiment_dict = json.load(f)\n",
    "\n",
    "# Read degree adverb dictionary\n",
    "with open('sentiment_dic/adv_dic.txt', 'r', encoding='utf-8') as f:\n",
    "    degree_dict = json.load(f)\n",
    "\n",
    "# Read negation dictionary\n",
    "with open('sentiment_dic/neg_dic.txt', 'r', encoding='utf-8') as f:\n",
    "    negation_dict = json.load(f)\n",
    "\n",
    "# Function to calculate sentiment score for a single sentence\n",
    "def sentiment_score(sen):\n",
    "    seg_list = sen.split()  # Convert text to list of words\n",
    "    emotion_score = 0  # Sentiment score\n",
    "    neg_count = 0  # Count of negation words\n",
    "    for i in range(len(seg_list)):\n",
    "        # Check for sentiment words\n",
    "        if seg_list[i] in sentiment_dict:\n",
    "            score = sentiment_dict[seg_list[i]]\n",
    "            degree_score = 1  # Default degree score is 1\n",
    "            negation_score = 1  # Default negation score is 1\n",
    "            # Look for degree and negation words within the valid distance\n",
    "            for j in range(max(0, i-3), i):\n",
    "                if seg_list[j] in degree_dict:\n",
    "                    degree_score *= degree_dict[seg_list[j]]\n",
    "                elif seg_list[j] in negation_dict:\n",
    "                    neg_count += 1\n",
    "            # Determine whether the sentiment score should be reversed based on the number of negation words\n",
    "            if neg_count % 2 == 0:\n",
    "                emotion_score += score * degree_score\n",
    "            else:\n",
    "                emotion_score -= score * degree_score\n",
    "            neg_count = 0  # Reset negation count\n",
    "    return round(emotion_score, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fdd7249",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.0\n"
     ]
    }
   ],
   "source": [
    "# Testing a typical positive comment\n",
    "sen = '位置 优秀 从来不 担心 车 没 地方 停 收费 贵 点儿 地下 停车场 设计 很 好 停车 极其 楼 基础设施 完善 逛起来 很 舒服 好评 点赞'\n",
    "score = sentiment_score(sen)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cf63f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2\n"
     ]
    }
   ],
   "source": [
    "# Testing a typical negative comment\n",
    "sen = '座位 没有 想 坐下 休息 地方 没有 逛累 顾客 滚蛋'\n",
    "score = sentiment_score(sen)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c3dcbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.02\n"
     ]
    }
   ],
   "source": [
    "# Testing a typical negative comment\n",
    "sen = '商场 整体 环境 还 行 有点 闷 月份 很 热 吃 饭 热 不行'\n",
    "score = sentiment_score(sen)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af606120",
   "metadata": {},
   "source": [
    "### Apply to previously processed comment text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "961c815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process four text files\n",
    "for i in range(1, 5):\n",
    "    # Read in text file\n",
    "    with open(f'../data/review/processed/reviews_{i}.txt', 'r', encoding='utf-8') as f:\n",
    "        reviews = f.readlines()\n",
    "\n",
    "    # Calculate sentiment score and polarity for each review\n",
    "    sentiment_scores = []\n",
    "    polarity = []\n",
    "    for review in reviews:\n",
    "        score = sentiment_score(review)\n",
    "        sentiment_scores.append(score)\n",
    "        if score < 0:\n",
    "            polarity.append('N') # If sentiment score is negative, assign 'N' for negative polarity\n",
    "        elif score >= 1:\n",
    "            polarity.append('P') # If sentiment score is positive, assign 'P' for positive polarity\n",
    "        else:\n",
    "            polarity.append('I') # If sentiment score is neutral, assign 'I' for neutral polarity\n",
    "\n",
    "    # Create a DataFrame for the reviews and their sentiment scores and polarity\n",
    "    df = pd.DataFrame(reviews, columns=['review_text'])\n",
    "    df['sentiment_score'] = sentiment_scores\n",
    "    df['polarity'] = polarity\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(f'../data/review/processed/with_sentiment_score/reviews_{i}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16691c3e",
   "metadata": {},
   "source": [
    "## 3. Use snownlp to help verify comment polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f128964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process four text files\n",
    "for i in range(1, 5):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(f'../data/review/processed/with_sentiment_score/reviews_{i}.csv')\n",
    "\n",
    "    # Read the review text file\n",
    "    with open(f'../data/review/reviews_{i}.txt', 'r', encoding='utf-8') as f:\n",
    "        reviews = f.readlines()\n",
    "\n",
    "    # Calculate sentiment score and polarity for each review and store in lists\n",
    "    sentiment_scores = []\n",
    "    polarities = []\n",
    "    for review in reviews:\n",
    "        s = SnowNLP(review)\n",
    "        sentiment_scores.append(s.sentiments)\n",
    "        if s.sentiments < 0.4:\n",
    "            polarities.append('N')\n",
    "        elif s.sentiments < 0.6:\n",
    "            polarities.append('I')\n",
    "        else:\n",
    "            polarities.append('P')\n",
    "\n",
    "    # Add the sentiment scores and polarities as new columns to the DataFrame\n",
    "    df['snownlp'] = sentiment_scores\n",
    "    df['polarity_snow_nlp'] = polarities\n",
    "\n",
    "    # Write the modified data back to the original CSV file\n",
    "    df.to_csv(f'../data/review/processed/with_sentiment_score/reviews_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c69e350",
   "metadata": {},
   "source": [
    "## 4. Further data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573fa52b",
   "metadata": {},
   "source": [
    "### Data Concatenation and Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3113c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 5):\n",
    "    # Read in the first CSV file\n",
    "    df_csv_1 = pd.read_csv(f'../data/review/processed/with_sentiment_score/reviews_{i}.csv')\n",
    "    # Read in the second CSV file\n",
    "    df_csv_2 = pd.read_csv(f'../data/review/reviews_{i}.csv')\n",
    "    # Concatenate the two DataFrames horizontally\n",
    "    df_concat = pd.concat([df_csv_1, df_csv_2], axis=1)\n",
    "    # Save the concatenated DataFrame as a CSV file\n",
    "    df_concat.to_csv(f'../data/review/analysis/reviews_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1321eb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store DataFrames\n",
    "dfs = {}\n",
    "\n",
    "# Loop through CSV files\n",
    "for i in range(1, 5):\n",
    "    # Read in a CSV file\n",
    "    df = pd.read_csv(f'../data/review/analysis/reviews_{i}.csv')\n",
    "    \n",
    "    # Get the name of the last column\n",
    "    last_col = df.columns[-1]\n",
    "    \n",
    "    # Move the last column to the beginning of the DataFrame\n",
    "    df = df.iloc[:, [df.shape[1]-1] + list(range(df.shape[1]-1))]\n",
    "    \n",
    "    # Rename the columns for consistency\n",
    "    df = df.rename(columns={\n",
    "        last_col: 'review',\n",
    "        'review_text': 'review_splitting',\n",
    "        'sentiment_score': 'senti_score',\n",
    "        'polarity': 'polarity',\n",
    "        'snownlp': 'senti_score_snownlp',\n",
    "        'polarity_snow_nlp': 'polarity_snownlp'\n",
    "    })\n",
    "    \n",
    "    # Add the DataFrame to the dictionary\n",
    "    dfs[f'reviews_{i}'] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "74b8fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove newline characters from the \"review\" and \"review_splitting\" columns\n",
    "for key in dfs.keys():\n",
    "    # Get the current DataFrame\n",
    "    review = dfs[key]\n",
    "    \n",
    "    # Select the columns to remove newline characters from\n",
    "    columns_to_clean = ['review', 'review_splitting']\n",
    "    \n",
    "    # Use str.replace() method to remove newline characters\n",
    "    for col in columns_to_clean:\n",
    "        review[col] = review[col].str.replace('\\n', '')\n",
    "        \n",
    "    # Save the modified DataFrame back to the original CSV file\n",
    "    review.to_csv(f'../data/review/analysis/{key}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a268c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign DataFrames to variables\n",
    "reviews_1_df = dfs['reviews_1']\n",
    "reviews_2_df = dfs['reviews_2']\n",
    "reviews_3_df = dfs['reviews_3']\n",
    "reviews_4_df = dfs['reviews_4']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b016d3",
   "metadata": {},
   "source": [
    "### Consistency Check for Polarity Labeling of Review Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5c8584f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_columns(df, col1, col2):\n",
    "    \"\"\"\n",
    "    Compares two columns of a pandas DataFrame row by row to check if they are equal, and calculates the ratio \n",
    "    of rows that are not equal.\n",
    "\n",
    "    Args:\n",
    "        df: pandas DataFrame, the data to compare.\n",
    "        col1: str, the name of the first column to compare.\n",
    "        col2: str, the name of the second column to compare.\n",
    "\n",
    "    Returns:\n",
    "        A percentage value of the number of rows that are not equal to the total number of rows in the DataFrame.\n",
    "    \"\"\"\n",
    "    # Create an empty list to store the row numbers where the two columns differ.\n",
    "    diff_rows = []\n",
    "    \n",
    "    # Iterate over each row of the DataFrame and compare the values of the two specified columns.\n",
    "    for i in range(len(df)):\n",
    "        if df.loc[i, col1] != df.loc[i, col2]:\n",
    "            # If the values are different, append the row number to the list of differing rows.\n",
    "            diff_rows.append(i)\n",
    "\n",
    "    # Calculate the percentage of rows that are different between the two columns.\n",
    "    diff_ratio = len(diff_rows) / len(df) * 100\n",
    "\n",
    "    # Print the results of the comparison.\n",
    "    if len(diff_rows) == 0:\n",
    "        print(f\"{col1} 和 {col2} 两列数据完全一致\")\n",
    "    else:\n",
    "        print(f\"{col1} 和 {col2} 两列数据共有 {len(diff_rows)} 行不一致，占全部行数的 {diff_ratio:.2f}%，即两种算法于该数据集表现出的一致性为 {100 - diff_ratio:.2f}%\")\n",
    "\n",
    "    # Return the percentage of differing rows.\n",
    "    return diff_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "15b00d31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity 和 polarity_snownlp 两列数据共有 1165 行不一致，占全部行数的 27.36%，即两种算法于该数据集表现出的一致性为 72.64%\n",
      "polarity 和 polarity_snownlp 两列数据共有 691 行不一致，占全部行数的 21.79%，即两种算法于该数据集表现出的一致性为 78.21%\n",
      "polarity 和 polarity_snownlp 两列数据共有 512 行不一致，占全部行数的 28.65%，即两种算法于该数据集表现出的一致性为 71.35%\n",
      "polarity 和 polarity_snownlp 两列数据共有 343 行不一致，占全部行数的 27.05%，即两种算法于该数据集表现出的一致性为 72.95%\n"
     ]
    }
   ],
   "source": [
    "# Calculate consistency ratio for four dataframes by comparing two columns using 'compare_columns' function.\n",
    "diff_ratio_1 = compare_columns(reviews_1_df, 'polarity', 'polarity_snownlp')\n",
    "diff_ratio_2 = compare_columns(reviews_2_df, 'polarity', 'polarity_snownlp')\n",
    "diff_ratio_3 = compare_columns(reviews_3_df, 'polarity', 'polarity_snownlp')\n",
    "diff_ratio_4 = compare_columns(reviews_4_df, 'polarity', 'polarity_snownlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9ff5ac6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall consistency: 73.79%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average difference ratio by adding the four ratios and dividing by four.\n",
    "diff_ratios = [diff_ratio_1, diff_ratio_2, diff_ratio_3, diff_ratio_4]\n",
    "avg_ratio = sum(diff_ratios) / len(diff_ratios)\n",
    "\n",
    "# Print the average difference ratio as a percentage with two decimal places.\n",
    "print(f\"Overall consistency: {100 - avg_ratio:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cd5c63",
   "metadata": {},
   "source": [
    "### Calculate the distribution of review text polarity (in percentages) for the four stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7830e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the percentage of each unique value in a given column of a DataFrame\n",
    "def calculate_percentages(df, column_name):\n",
    "    # Count the number of occurrences of each unique value in the specified column\n",
    "    counts = df[column_name].value_counts()\n",
    "\n",
    "    # Calculate the percentage of each unique value in relation to the total number of values in the column\n",
    "    percentages = counts / counts.sum() * 100\n",
    "\n",
    "    # Return the percentages\n",
    "    return percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3b95b731",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "北京APM：\n",
      "P    75.598873\n",
      "N    13.785815\n",
      "I    10.615312\n",
      "Name: polarity, dtype: float64\n",
      "王府中环：\n",
      "P    82.749921\n",
      "N    10.028382\n",
      "I     7.221697\n",
      "Name: polarity, dtype: float64\n",
      "王府井百货：\n",
      "P    73.363179\n",
      "N    15.836598\n",
      "I    10.800224\n",
      "Name: polarity, dtype: float64\n",
      "东方新天地：\n",
      "P    73.974763\n",
      "N    14.826498\n",
      "I    11.198738\n",
      "Name: polarity, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Call the calculate_percentages function for each DataFrame to get the percentage of each unique value in the \"polarity\" column\n",
    "percentages_1 = calculate_percentages(reviews_1_df, \"polarity\")\n",
    "percentages_2 = calculate_percentages(reviews_2_df, \"polarity\")\n",
    "percentages_3 = calculate_percentages(reviews_3_df, \"polarity\")\n",
    "percentages_4 = calculate_percentages(reviews_4_df, \"polarity\")\n",
    "\n",
    "# Print the results\n",
    "print(\"北京APM：\")\n",
    "print(percentages_1)\n",
    "\n",
    "print(\"王府中环：\")\n",
    "print(percentages_2)\n",
    "\n",
    "print(\"王府井百货：\")\n",
    "print(percentages_3)\n",
    "\n",
    "print(\"东方新天地：\")\n",
    "print(percentages_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "857b3178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "P    3219\n",
       "N     587\n",
       "I     452\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_1_df[\"polarity\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4319859f",
   "metadata": {},
   "source": [
    "### Calculate the Mean value of sentiment_score for the four stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a73e0ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 'senti_score' column for reviews_1: 4.27\n",
      "Mean of 'senti_score' column for reviews_2: 5.00\n",
      "Mean of 'senti_score' column for reviews_3: 3.22\n",
      "Mean of 'senti_score' column for reviews_4: 3.33\n"
     ]
    }
   ],
   "source": [
    "# Loop through each DataFrame in the dictionary and calculate the average of the 'senti_score' column\n",
    "for name, df in dfs.items():\n",
    "    senti_score_mean = df['senti_score'].mean()\n",
    "    print(f\"Mean of 'senti_score' column for {name}: {senti_score_mean:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce5ec0e",
   "metadata": {},
   "source": [
    "## 5. Store comment text in categories based on polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "481ac315",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for key in dfs.keys():\n",
    "#     # Get the current DataFrame\n",
    "#     review = dfs[key]\n",
    "    \n",
    "#     # Group the DataFrame by the \"polarity\" column\n",
    "#     groups = review.groupby('polarity')\n",
    "    \n",
    "#     # Extract DataFrames for \"P\" and \"N\" polarities from the groups\n",
    "#     review_p = groups.get_group('P')\n",
    "#     review_n = groups.get_group('N')\n",
    "    \n",
    "#     # Select only the \"review\" column and write it to a txt file\n",
    "#     review_p['review'].to_csv(f'../data/review/analysis/polarity/{key}_polarity_P.txt', index=False, header=False)\n",
    "#     review_n['review'].to_csv(f'../data/review/analysis/polarity/{key}_polarity_N.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "59e15304",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dfs.keys():\n",
    "    # Get the current DataFrame\n",
    "    review = dfs[key]\n",
    "    \n",
    "    # Group the DataFrame by the \"polarity\" column\n",
    "    groups = review.groupby('polarity')\n",
    "    \n",
    "    # Extract DataFrames for \"P\" and \"N\" polarities from the groups\n",
    "    review_p = groups.get_group('P')\n",
    "    review_n = groups.get_group('N')\n",
    "    \n",
    "    # Select only the \"review splitting\" column and write it to a txt file\n",
    "    review_p['review_splitting'].to_csv(f'../data/review/analysis/polarity/{key}_polarity_P.txt', index=False, header=False)\n",
    "    review_n['review_splitting'].to_csv(f'../data/review/analysis/polarity/{key}_polarity_N.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d830262",
   "metadata": {},
   "source": [
    "## 6. Splitting text into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f40085d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the file path for the stop words\n",
    "# stopword_path = 'stopwords/stopwords.txt'\n",
    "\n",
    "# # Create a set of stop words by opening the file and using a set comprehension\n",
    "# with open(stopword_path, 'r', encoding='utf-8') as f:\n",
    "#     stopwords = {word.strip() for word in f}\n",
    "\n",
    "# # Load the custom dictionary\n",
    "# custom_dict_path = 'stopwords/custom_dict.txt'\n",
    "# jieba.load_userdict(custom_dict_path)\n",
    "\n",
    "# # Define a function to process each review text\n",
    "# def process_text(text):\n",
    "#     # Tokenize the text with Jieba and enable HMM\n",
    "#     tokens = jieba.cut(text, cut_all=False, HMM=True)\n",
    "#     # Remove stop words and punctuation\n",
    "#     tokens = [token for token in tokens if token not in stopwords and token.strip() and not token.isnumeric()]\n",
    "#     # Return the processed text as a string\n",
    "#     return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d0d48067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the directory path to the review files\n",
    "# dir_path = '../data/review/analysis/polarity'\n",
    "\n",
    "# # Loop through each file in the directory\n",
    "# for filename in os.listdir(dir_path):\n",
    "#     # Check if the file is a txt file\n",
    "#     if filename.endswith('.txt'):\n",
    "#         # Construct the file path\n",
    "#         file_path = os.path.join(dir_path, filename)\n",
    "#         # Read in the contents of the file\n",
    "#         with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#             text = f.read()\n",
    "#         # Process the text using the process_text function\n",
    "#         processed_text = process_text(text)\n",
    "#         # Write the processed text back to the file\n",
    "#         with open(file_path, 'w', encoding='utf-8') as f:\n",
    "#             f.write(processed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
